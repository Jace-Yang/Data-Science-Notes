## 文本分类基础

文本数据属于非结构化数据，对于非结构化数据的处理，首先要转化为结构化数据，常用转化方式有以下四种

• One-hot编码：将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值

• Bag of Words：每个文档的字/词可以使用其出现次数来进行表示

• N-gram：与Bag of Words策略类似，加入了相邻单词组合成为的新单词（组合数为N），并进行计数

• TF-IDF：TF代表词语频率，IDF代表逆文档频率

​	TF = 该词语在当前文档出现的次数 / 当前文档中词语的总数

​	IDF = log (文档总数 / 出现该词语的文档总数)

通过以上四种方式将非结构化文本数据转化为结构化数据后，构建机器学习分类模型，例如逻辑回归、决策树、支持向量机等，可以预测不同文档所属类别

## FastText

FastText是Facebook提出的一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。

一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。

fastText模型有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入是多个单词及其n-gram特征，这些特征用来表示单个文档，输出是文档对应的类标，隐含层都是对多个词向量的叠加平均。

<img src="https://pic2.zhimg.com/80/v2-7f38f23e98ee89d21fd16e34d5f07d69_720w.jpg" alt="img" style="zoom:67%;" />

## TextCNN/TextRNN

### TextCNN

TextCNN模型可以使用卷积神经网络CNN进行文本特征提取，不同大小的卷积核可以提取不同的N-Gram特征，卷积结果通过MaxPooling层提取最大特征值，并转换成一个向量作为文本的表示

如下图所示，横轴代表不同的单词，纵轴代表词向量的维度，通过一个卷积层过滤后提取了文本的N-Gram特征，之后过池化层，提取最大特征值，过一个FC层，并用softmax输出文本类别，使用梯度下降训练这个网络

<img src="https://camo.githubusercontent.com/3d1321f837157424eba151bdd25dd6d348a37d54f211444bede4236682a1ab23/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230353933323732302e6a706567" alt="img" style="zoom:67%;" />

### TextRNN

TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。

如下图所示，使用一个双向LSTM对文本数据进行训练，LSTM网络采用Nto1的模型，输出层过一个FC层后使用Softmax输出文本类别，使用梯度下降法可以训练这个网络

<img src="https://camo.githubusercontent.com/691e59078e12a15ac6443d408db249f263108373e45b4cf9fefe6a8c86d3dba9/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231303830363439322e706e67" alt="5" style="zoom: 50%;" />

## Transformer

Transformer是采用编码器、解码器构建的神经网络模型，在原始论文中，作者首先使用6个编码器对原始文本做处理，接下来使用6个解码器对第6个编码器的输出进行解码，最终用于机器翻译、文本分类等任务

下图所示的网络是一个简单的Encoder结构，用于训练的文本信息首先过一个Self-Attention层，Attention后的结果过一个前向神经网络层后输出，在注意力层的每个子层中，都含有残差连接，并加入了层标准化

<img src="https://camo.githubusercontent.com/f769b0719f291d41ed402272fd561a4e780b83d824329a6237c946d5cd63d327/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313935353731332e706e67" alt="img" style="zoom:67%;" />

## 预训练语言模型

基于预训练语言模型的词表示由于可以建模上下文信息，进而解决传统静态词向量不能建模“一词多义”语言现象的问题。常用的预训练语言模型包括ELMo模型、GPT模型和BERT模型

ELMo基于两个单向LSTM，将从左到右和从右到左两个方向的隐藏层向量表示拼接学习上下文词嵌入。而GPT用Transformer代替LSTM作为编码器，首先进行了语言模型预训练，然后在下游任务微调模型参数。但GPT由于仅使用了单向语言模型，因此难以建模上下文信息。为了解决以上问题，研究者们提出了BERT，BERT模型结构如下图所示，它是一个基于Transformer的多层Encoder，通过执行一系列预训练，进而得到深层的上下文表示

![bert_elmo](https://camo.githubusercontent.com/f204fd31e2ff16d5c99be41cb74465d4308c9e57b12622ce09e4910de8be30f7/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313331363136372e706e67)