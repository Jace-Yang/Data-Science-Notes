## 文本分类基础

文本数据属于非结构化数据，对于非结构化数据的处理，首先要转化为结构化数据，常用转化方式有以下四种

• One-hot编码：将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值

• Bag of Words：每个文档的字/词可以使用其出现次数来进行表示

• N-gram：与Bag of Words策略类似，加入了相邻单词组合成为的新单词（组合数为N），并进行计数

• TF-IDF：TF代表词语频率，IDF代表逆文档频率

​	TF = 该词语在当前文档出现的次数 / 当前文档中词语的总数

​	IDF = log (文档总数 / 出现该词语的文档总数)

通过以上四种方式将非结构化文本数据转化为结构化数据后，构建机器学习分类模型，例如逻辑回归、决策树、支持向量机等，可以预测不同文档所属类别

## FastText

FastText是Facebook提出的一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。

一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。

fastText模型有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入是多个单词及其n-gram特征，这些特征用来表示单个文档，输出是文档对应的类标，隐含层都是对多个词向量的叠加平均。

<img src="https://pic2.zhimg.com/80/v2-7f38f23e98ee89d21fd16e34d5f07d69_720w.jpg" alt="img" style="zoom:67%;" />

## TextCNN/TextRNN

### TextCNN

TextCNN模型可以使用卷积神经网络CNN进行文本特征提取，不同大小的卷积核可以提取不同的N-Gram特征，卷积结果通过MaxPooling层提取最大特征值，并转换成一个向量作为文本的表示

如下图所示，横轴代表不同的单词，纵轴代表词向量的维度，通过一个卷积层过滤后提取了文本的N-Gram特征，之后过池化层，提取最大特征值，过一个FC层，并用softmax输出文本类别，使用梯度下降训练这个网络

<img src="https://camo.githubusercontent.com/3d1321f837157424eba151bdd25dd6d348a37d54f211444bede4236682a1ab23/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230353933323732302e6a706567" alt="img" style="zoom:67%;" />

### TextRNN

TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。

如下图所示，使用一个双向LSTM对文本数据进行训练，LSTM网络采用Nto1的模型，输出层过一个FC层后使用Softmax输出文本类别，使用梯度下降法可以训练这个网络

<img src="https://camo.githubusercontent.com/691e59078e12a15ac6443d408db249f263108373e45b4cf9fefe6a8c86d3dba9/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231303830363439322e706e67" alt="5" style="zoom: 50%;" />

## Transformer

Transformer是采用编码器、解码器构建的神经网络模型，在原始论文中，作者首先使用6个编码器对原始文本做处理，接下来使用6个解码器对第6个编码器的输出进行解码，最终用于机器翻译、文本分类等任务

下图所示的网络是一个简单的Encoder结构，用于训练的文本信息首先过一个Self-Attention层，Attention后的结果过一个前向神经网络层后输出，在注意力层的每个子层中，都含有残差连接，并加入了层标准化

<img src="https://camo.githubusercontent.com/f769b0719f291d41ed402272fd561a4e780b83d824329a6237c946d5cd63d327/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343231313935353731332e706e67" alt="img" style="zoom:67%;" />

## 预训练语言模型

基于预训练语言模型的词表示由于可以建模上下文信息，进而解决传统静态词向量不能建模“一词多义”语言现象的问题。常用的预训练语言模型包括ELMo模型、GPT模型和BERT模型

### ELMO

使用word2vec等词嵌入得到的词向量是静态词向量，每个词对应一个向量，无法解决一词多义的问题。ELMO模型可以解决一词多义和随着语言环境改变，词义改变的问题

ELMO的训练过程如下：

• 在大语料上以language model为目标训练出bidirectional LSTM模型

• 利用LSTM产生词语的表征

<img src="https://camo.githubusercontent.com/88944a9933dd03bf8b2253c6484b89085d58d328182c1134aaab354afcd7333d/68747470733a2f2f7069726374757265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f696d672f312e706e67" alt="img" style="zoom:67%;" />

### GPT

GPT是OPENAI提出的基于预训练的模型，模型分为两个部分，无监督与训练以及各任务微调

无监督与训练使用语言模型最大化词语出现的条件概率；GPT使用一个多层的transformer decoder作为语言模型，并将decoder的输出经过一个softmax，来得到目标词的输出分布
$$
L(U)=\sum_i \log P(u_i|u_{i-k},...,u_{i-1};\theta)
$$
<img src="https://camo.githubusercontent.com/2a6ffa1e01a9e1a770950cf9f969c4be0531e4f0774897cc5edf8d4c2267fac4/68747470733a2f2f7069726374757265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f696d672f322e706e67" alt="img" style="zoom:67%;" />

### BERT

BERT模型与OpenAI GPT的区别就在于采用了Transformer Encoder，也就是每个时刻的Attention计算都能够得到全部时刻的输入，而OpenAI GPT采用了Transformer Decoder，每个时刻的Attention计算只能依赖于该时刻前的所有时刻的输入，因为OpenAI GPT是采用了单向语言模型

<img src="https://camo.githubusercontent.com/080d33065ab9db0501bd0f4b2188a26006e468b69166a165866e84e0160d40a2/68747470733a2f2f7069726374757265732e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f696d672f352e706e67" alt="img" style="zoom:50%;" />

