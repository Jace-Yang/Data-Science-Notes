## 简介

### NLP任务

- 文本分类：对单个、两个或者多段文本进行分类
- 序列标注：对文本序列中的token、字或者词进行分类
- 问答任务：抽取式问答和多选问答，抽取式问答根据**问题**从一段给定的文本中找到**答案**，答案必须是给定文本的一小段文字；多选式问答从多个选项中选出一个正确答案
- 生成任务：根据已有的一段文字生成一个字通常叫做语言模型；根据一大段文字生成一小段总结性文字通常叫做摘要生成；将源语言比如中文句子翻译成目标语言比如英语通常叫做机器翻译

## Attention

### seq2seq模型

模型是由编码器（Encoder）和解码器（Decoder）组成的。其中，编码器会处理输入序列中的每个元素，把这些信息转换为一个向量，称为上下文（context）。当我们处理完整个输入序列后，编码器把上下文发送给解码器，解码器开始逐项生成输出序列中的元素

在发明transformer之前，常用的编码器和解码器是RNN，将sequence的每个步骤当作一个输入，最终输出一个向量作为解码器的输入

<img src="/Users/shawnzhang/Library/Application Support/typora-user-images/image-20210801132846492.png" alt="image-20210801132846492"  />

### Attention机制

使用RNN做编码器和解码器的seq2seq模型不擅长处理长文本，因此发明了Attention机制

- 编码器会把更多的数据传递给解码器：编码器把所有时间步的 hidden state=传递给解码器，而不是只传递最后一个 hidden state
- 注意力模型的解码器在产生输出之前，做了额外的处理
  - 查看所有接收到的编码器的 hidden state。其中，编码器中每个 hidden state都对应到输入句子中一个单词
  - 给每个 hidden state一个分数，将每个 hidden state乘以经过 softmax 的对应的分数，从而高分对应的 hidden state会被放大，而低分对应的 hidden state会被缩小

![image-20210801133143316](/Users/shawnzhang/Library/Application Support/typora-user-images/image-20210801133143316.png)

1. 注意力模型的解码器 RNN 的输入包括：一个embedding 向量，和一个初始化好的解码器 hidden state
2. RNN 处理上述的 2 个输入，产生一个输出和一个新的 hidden state，其中输出会被忽略
3. 注意力的步骤：我们使用编码器的 hidden state 和 h4 向量来计算这个时间步的上下文向量C4
4. 我们把 h4 和 C4 拼接起来，得到一个向量
5. 我们把这个向量输入一个前馈神经网络（这个网络是和整个模型一起训练的）
6. 前馈神经网络的输出的输出表示这个时间步输出的单词
7. 在下一个时间步重复这个步骤

![image-20210801133608496](/Users/shawnzhang/Library/Application Support/typora-user-images/image-20210801133608496.png)

## Transformer

 Transformer 可以拆分为 2 部分，编码部分和解码部分；编码部分是多层的编码器(Encoder)组成，解码部分也是由多层的解码器(Decoder)组成

### Encoder

编码器 在结构上都是一样的，但它们的权重参数是不同的。每一个编码器里面，可以分为 2 层

- Self-Attention Layer：处理一个词的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息
- Feed Forward Neural Network

<img src="https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-x-encoder.png" alt="输入encoder" style="zoom:67%;" />

### Self-Attention

Single Attention

- 对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量

- 计算 Attention Score：计算单词对应的 Query 向量和其他位置的每个词的 Key 向量的点积

- 把每个分数除以 $\sqrt{d_{key}}$（dkey是 Key 向量的长度），为了在反向传播时，求取梯度更加稳定

- 把这些分数经过一个 Softmax 层，将分数归一化

- 得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘

- 把上一步得到的向量相加，就得到了 Self Attention 层在这个位置的输出

<img src="https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-qkv-multi.png" alt="img" style="zoom:50%;" />

<img src="https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-attention-output.webp" alt="输出" style="zoom:50%;" />

Multi-head Attention

- 多头注意力机制赋予 attention 层多个“子表示空间”，创建多组Q K V矩阵
- 对每组Q K V矩阵分别计算Z值
- 将计算出的多组Z值合并成一个向量，再乘另一个Q矩阵得到最终的Z值

![放在一起](https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-put-together.webp)

### Decoder

解码（decoding ）阶段的每一个时间步都输出一个单词。编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。

Encoder-Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，不同之处是：Encoder-Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于解码器最终的输出。

![image-20210801174001185](/Users/shawnzhang/Library/Application Support/typora-user-images/image-20210801174001185.png)

### 输出层

Decoder 最终的输出是一个向量，其中每个元素是浮点数，接下来使用线性层和softmax层可以得到希望输出的单词序列

线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量。然后，Softmax 层会把这些分数转换为概率，选择最高概率的那个数字对应的词，就是这个时间步的输出单词。

<img src="https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-linear.png" alt="线性层" style="zoom:75%;" />

### 模型训练

我们的模型需要输出多个概率分布，每个概率分布都是一个向量，长度是 vocab_size，代表在这个位置单词表中单词的概率。我们的优化目标是输出的概率分布和真实分布一致，通过两个概率分布之差构建损失函数。

greedy decoding: 模型是从概率分布中选择概率最大的词，并且丢弃其他词

Beam search: 每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程，即选取某个步骤概率最大的N个词(N称为beam size)，根据第一个词计算第二个位置的词的概率分布，再取出 N个概率最高的词，重复该过程