## 简介

### NLP任务

- 文本分类：对单个、两个或者多段文本进行分类
- 序列标注：对文本序列中的token、字或者词进行分类
- 问答任务：抽取式问答和多选问答，抽取式问答根据**问题**从一段给定的文本中找到**答案**，答案必须是给定文本的一小段文字；多选式问答从多个选项中选出一个正确答案
- 生成任务：根据已有的一段文字生成一个字通常叫做语言模型；根据一大段文字生成一小段总结性文字通常叫做摘要生成；将源语言比如中文句子翻译成目标语言比如英语通常叫做机器翻译

## Attention

### seq2seq模型

模型是由编码器（Encoder）和解码器（Decoder）组成的。其中，编码器会处理输入序列中的每个元素，把这些信息转换为一个向量，称为上下文（context）。当我们处理完整个输入序列后，编码器把上下文发送给解码器，解码器开始逐项生成输出序列中的元素

在发明transformer之前，常用的编码器和解码器是RNN，将sequence的每个步骤当作一个输入，最终输出一个向量作为解码器的输入

<img src="https://i.loli.net/2021/08/02/EJUhg5e7mtHpIaL.png" alt="image-20210801132846492"  />

### Attention机制

使用RNN做编码器和解码器的seq2seq模型不擅长处理长文本，因此发明了Attention机制

- 编码器会把更多的数据传递给解码器：编码器把所有时间步的 hidden state=传递给解码器，而不是只传递最后一个 hidden state
- 注意力模型的解码器在产生输出之前，做了额外的处理
  - 查看所有接收到的编码器的 hidden state。其中，编码器中每个 hidden state都对应到输入句子中一个单词
  - 给每个 hidden state一个分数，将每个 hidden state乘以经过 softmax 的对应的分数，从而高分对应的 hidden state会被放大，而低分对应的 hidden state会被缩小

![image-20210801133143316](https://i.loli.net/2021/08/02/NRtgJu6OphvwfoA.png)

1. 注意力模型的解码器 RNN 的输入包括：一个embedding 向量，和一个初始化好的解码器 hidden state
2. RNN 处理上述的 2 个输入，产生一个输出和一个新的 hidden state，其中输出会被忽略
3. 注意力的步骤：我们使用编码器的 hidden state 和 h4 向量来计算这个时间步的上下文向量C4
4. 我们把 h4 和 C4 拼接起来，得到一个向量
5. 我们把这个向量输入一个前馈神经网络（这个网络是和整个模型一起训练的）
6. 前馈神经网络的输出的输出表示这个时间步输出的单词
7. 在下一个时间步重复这个步骤

![image-20210801133608496](https://i.loli.net/2021/08/02/nA1BrGh5DlfoUeR.png)

## Transformer

 Transformer 可以拆分为 2 部分，编码部分和解码部分；编码部分是多层的编码器(Encoder)组成，解码部分也是由多层的解码器(Decoder)组成

### Encoder

编码器 在结构上都是一样的，但它们的权重参数是不同的。每一个编码器里面，可以分为 2 层

- Self-Attention Layer：处理一个词的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息
- Feed Forward Neural Network

<img src="https://i.loli.net/2021/08/02/YeipAzUr9aOC1Fb.png" alt="输入encoder" style="zoom:67%;" />

### Self-Attention

Single Attention

- 对输入编码器的每个词向量，都创建 3 个向量，分别是：Query 向量，Key 向量，Value 向量

- 计算 Attention Score：计算单词对应的 Query 向量和其他位置的每个词的 Key 向量的点积

- 把每个分数除以 $\sqrt{d_{key}}$（dkey是 Key 向量的长度），为了在反向传播时，求取梯度更加稳定

- 把这些分数经过一个 Softmax 层，将分数归一化

- 得到每个位置的分数后，将每个分数分别与每个 Value 向量相乘

- 把上一步得到的向量相加，就得到了 Self Attention 层在这个位置的输出

<img src="https://i.loli.net/2021/08/02/T7kBXNpvwE2fMQD.png" alt="img" style="zoom:50%;" />

<img src="https://i.loli.net/2021/08/02/o5epOtZEQ3vnLzH.png" alt="image-20210802141241305" style="zoom:50%;" />

Multi-head Attention

- 多头注意力机制赋予 attention 层多个“子表示空间”，创建多组Q K V矩阵
- 对每组Q K V矩阵分别计算Z值
- 将计算出的多组Z值合并成一个向量，再乘另一个Q矩阵得到最终的Z值

<img src="https://i.loli.net/2021/08/02/TSA9yxJihna8IUP.png" alt="image-20210802141333299" style="zoom: 67%;" />

### Decoder

解码（decoding ）阶段的每一个时间步都输出一个单词。编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量 K 和 V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中中输入序列的合适位置。

Encoder-Decoder Attention层的原理和多头注意力（multiheaded Self Attention）机制类似，不同之处是：Encoder-Decoder Attention层是使用前一层的输出来构造 Query 矩阵，而 Key 矩阵和 Value 矩阵来自于解码器最终的输出。

![image-20210801174001185](https://i.loli.net/2021/08/02/E1hdnzojfW3KFAi.png)

### 输出层

Decoder 最终的输出是一个向量，其中每个元素是浮点数，接下来使用线性层和softmax层可以得到希望输出的单词序列

线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为 logits 向量。然后，Softmax 层会把这些分数转换为概率，选择最高概率的那个数字对应的词，就是这个时间步的输出单词。

<img src="https://i.loli.net/2021/08/02/VTl1ZdUptiHPxMv.png" alt="image-20210802141515434" style="zoom:50%;" />

### 模型训练

我们的模型需要输出多个概率分布，每个概率分布都是一个向量，长度是 vocab_size，代表在这个位置单词表中单词的概率。我们的优化目标是输出的概率分布和真实分布一致，通过两个概率分布之差构建损失函数。

greedy decoding: 模型是从概率分布中选择概率最大的词，并且丢弃其他词

Beam search: 每个时间步保留两个最高概率的输出词，然后在下一个时间步，重复执行这个过程，即选取某个步骤概率最大的N个词(N称为beam size)，根据第一个词计算第二个位置的词的概率分布，再取出 N个概率最高的词，重复该过程

## BERT

### 模型输入

就像 Transformer 中普通的 Encoder 一样，BERT 将一串单词作为输入，这些单词在 Encoder 的栈中不断向上流动。每一层都会经过 Self Attention 层，并通过一个前馈神经网络，然后将结果传给下一个 Encoder

<img src="https://i.loli.net/2021/08/02/RMEfSOJraqFWC5t.png" alt="image-20210802141639362" style="zoom:50%;" />

### 模型输出

每个位置输出一个大小为 hidden_size 的向量，输出向量可以作为后面分类器的输入。

<img src="https://i.loli.net/2021/08/02/micfFsJotbSXIQC.png" alt="image-20210802141709226" style="zoom:50%;" />

## GPT

### 语言模型

语言模型是一个机器学习模型，它可以根据句子的一部分预测下一个词。最著名的语言模型就是手机键盘，它可以根据你输入的内容，提示下一个单词。而GPT-2 是使用 Transformer 的 Decoder 模块构建的，和传统的语言模型一样，一次输出一个 token，在产生每个 token 之后，将这个 token 添加到输入的序列中，形成一个新序列。然后这个新序列成为模型在下一个时间步的输入。这是一种叫“自回归（auto-regression）”的思想。

### 整体结构

<img src="https://i.loli.net/2021/08/04/VYDsOSXuMAReWTC.png" alt="image-20210804142658390" style="zoom: 67%;" />

把一个单词输入到 Transformer 的第一个模块，寻找这个单词的 embedding，并且添加第一个位置的位置编码向量

<img src="https://i.loli.net/2021/08/04/CfclGpahJ9Pq8XU.png" alt="image-20210804143308484" style="zoom:67%;" />

第一个Decoder模块处理输入的token，首先通过 Self Attention 层，然后通过神经网络层。一旦 Transformer 的第一个模块处理了 token，会得到一个结果向量，这个结果向量会被发送到堆栈的下一个模块处理。每个模块的处理过程都是相同的，不过每个模块都有自己的 Self Attention 和神经网络层。

<img src="https://i.loli.net/2021/08/04/kmqfGxULZCaOSQH.png" alt="image-20210804143837392" style="zoom:67%;" />

当模型顶部的模块产生输出向量时，模型会将这个向量乘以嵌入矩阵，相乘的结果被解释为模型词汇表中每个词的分数。把分数作为单词的概率，从整个列表中选择一个单词，完成一次迭代，之后进行下一轮迭代，知道选出所有词为止

### Masked Self Attention

<img src="https://i.loli.net/2021/08/04/7Vr4TNnyIL9Ghtk.png" alt="image-20210804180615517" style="zoom:67%;" />

Self Attention 层会屏蔽未来的 token。具体来说，通过改变 Self Attention 的计算，阻止来自被计算位置右边的 token

假设模型只有 2 个 token 作为输入，我们正在观察（处理）第二个 token。在这种情况下，最后 2 个 token 是被屏蔽（masked）的。所以模型会干扰评分的步骤。它基本上总是把未来的 token 评分为 0，因此模型不能看到未来的词。这个屏蔽（masking）经常用一个矩阵来实现，称为 attention mask。
