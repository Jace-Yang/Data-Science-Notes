## Outline

Week 1: introduction; data and presentations; descriptive statistics

Week 2: sample spaces, set operations, axioms probability; conditional probability 

Week 3: random variables (rv), pmfs/pdfs/cdfs; independence, expected value, joint rvs

Week 4: conditional rvs, variance/covariance/correlations; inequalities, weak law of large numbers (LLN)

Week 5: discrete named distributions (Bernoulli, binomial, hypergeometric, Poisson, etc)

Week 6: continuous named distributions (normal, exponential, uniform,Chi-square); Poisson process

Week 7: parameters, likelihood, maximum likelihood estimation (MLE), central limit theorem (CLT) continuity corrections, general confidence intervals

Week 8: z-scores, z-intervals, prediction intervals, sampling distribution of the mean estimator; t-intervals, binomial intervals

Week 9: hypothesis testing, z-tests under different situations 

Week 10: one sample t-tests, two samples, paired, binomial tests

Week 11: simple regression overview, MLE estimates and their distributions; multiple regression optimization

Week 12: weighted least squares, multiple linear regression, one way/two way analysis of variance (ANOV A)

Week 13: Chi-square goodness of fit; review

## Fundamental

### Data Collection

- Collect data from real word, sensor, etc.

- Use statistical theory to design an appropriate experiment to generate data (Random Experiment)

### Statistics

- Descriptive statistics: description and summarization of data

- Inferential statistics: statistics for drawing of conclusions

### Population and sample

- population: a total collection of elements

- sample: subgroup of a population

In practice, a given sample generally cannot be assumed to be representative of a population unless that sample has been chosen in a random manner. 

### Descriptive Statistics

- Statistics
  - Frequency: times of occurrences, can be drawed into tables and graphs
  - Relative Frequency: frequency/total, can be described as tables and graphs
  - Sample Mean: the arithmetic average of values; sample mean makes use of all the data values and is affected by extreme values that are much larger or smaller than the others
  - Sample Median: the value in position (*n* + 1)/2; if *n* is even, it is the average of the values in positions *n*/2 and *n*/2 + 1; the sample median makes use of only one or two of the middle values and is thus not affected by extreme values
  - Sample Mode: the value that occurs with the greatest frequency
  - Sample Variance: $s^2=\sum_{i=1}^n(x_i-\bar x)^2/(n-1)$
  - Sample Standard Deviation:   $s=\sqrt{\sum_{i=1}^n(x_i-\bar x)^2/(n-1)}$
  - Sample Percentile: at least 100*p* percent of the data are less than or equal to it and at least 100(1 − *p*) percent are greater than or equal to it

- Graph

  line graph*: plots the distinct data values on the horizontal axis and indicates their frequencies by the heights of vertical lines

<img src="https://i.loli.net/2021/08/03/r6f7OXvjzBysDmi.png" alt="image-20210725132021997" style="zoom:25%;" />

bar graph*: lines in a line graph are given added thickness

<img src="https://i.loli.net/2021/08/03/zu3dkCpZD2eiOxK.png" alt="image-20210725132047571" style="zoom: 25%;" />

*frequency polygon*: plots the frequencies of the different data values on the vertical axis, and then 	connects the plotted points with straight lines

<img src="https://i.loli.net/2021/08/03/PcKB3TWx4kVQCfn.png" alt="image-20210725132109777" style="zoom:25%;" />

*pie chart*: indicate relative frequencies when the data are not numerical

<img src="https://i.loli.net/2021/08/03/TY1qfDdj9sRIQgE.png" alt="image-20210725132313820" style="zoom:25%;" />

*frequency histogram*: a bar graph plot of class data representing frequency

<img src="https://i.loli.net/2021/08/03/eQMw1v9nO7oBbYC.png" alt="image-20210725133002966" style="zoom:25%;" />

*relative frequency histogram*: a bar graph plot of class data representing relative frequency

*stem and leaf plot*: dividing each data value into two parts — its stem and its leaf

<img src="https://i.loli.net/2021/08/03/Vf1wojqmRcvl5Fb.png" alt="image-20210725133055239" style="zoom:25%;" />

*box plot*: 

<img src="https://i.loli.net/2021/08/03/FykGmlfaPcWtOq5.png" alt="image-20210725154041422" style="zoom:25%;" />

### Chebyshev's inequality

- Two-Sided chebyshev's inequality

$$
S_k=\{i, 1\le i\le n:|x_i-\bar x|<ks\}
$$

$$
\frac{|S_k|}{n}\ge 1-\frac{n-1}{nk^2}\ge 1-\frac{1}{k^2}
$$

- One-Sided chebyshev's inequality

$$
N_k=\{i, 1\le i\le n:x_i-\bar x\ge ks\}
$$

$$
\frac{N(k)}{n}\le \frac{1}{1+k^2}
$$

### Normal Data

- normal histograms

<img src="https://i.loli.net/2021/08/03/HUOQtzA3ey5kXaj.png" alt="image-20210725160122767" style="zoom:25%;" />

- approximatedly normal

<img src="https://i.loli.net/2021/08/03/cIMksd6wjrZHWbK.png" alt="image-20210725160204936" style="zoom:25%;" />

- skewed to the left

<img src="https://i.loli.net/2021/08/04/imH1VU87vINjOr2.png" alt="image-20210725160227931" style="zoom:25%;" />

- skewed to the right

<img src="https://i.loli.net/2021/08/04/gTyaECiBHuhAlX3.png" alt="image-20210725160253390" style="zoom:25%;" />

- bimodal

<img src="https://i.loli.net/2021/08/04/gx6K3oLamYtqJQV.png" alt="image-20210725160342698" style="zoom:25%;" />

### Paired Data

- sample correlation coefficient: 

$$
r=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2\sum_{i=1}^n(y_i-\bar y)^2}}
$$

When *r* > 0 we say that the sample data pairs are *positively correlated*, and when *r* < 0 we say that they are *negatively correlated*

Correlation measures association, not causation, we can not conclude that high correlation means one factor contributes to another

## Probability

Frequency interpretation: the probability of the outcome will be observable as being the proportion of the experiments that result in the outcome

Bayesian interpretation: the probability of an outcome is considered a statement about the beliefs of the person who is quoting the probability

### Sample Space

- sample space: set of all possible outcomes of an experiment

- event: any subset *E* of the sample space
- set operation
  - Intersaction: $E \cap F$
  - Union: $E \cup F$
  - Complement: $E^c$
  - mutually exclusive: $E \cap F=\phi$
  - E is contained in F: $E \subset F$
  - communative law: $E \cup F=F\cup E$
  - associative law: $(E\cup F)\cup G=E\cup (F\cup G)$
  - distributive law: $(E\cup F)\cap G=(E\cap G) \cup (F\cap G)$

![image-20210725182157150](https://i.loli.net/2021/08/04/G9rbVN2Xszoxmea.png)

### Conditional Probability

- general formula

$$
P(E|F)=\frac{P(EF)}{P(F)}
$$

- Bayes formula

$$
P(E)=P(EF)+P(EF^c)=P(E|F)P(F)+P(E|F^c)(1-P(F))
$$

$$
P(E)=\sum_{i=1}^nP(EF_i)=\sum_{i=1}^nP(E|F_i)P(F_i)
$$

$$
P(F_j|E)=\frac{P(E|F_j)P(F_j)}{\sum_{i=1}^nP(E|F_i)P(F_i)}
$$

### Independence

- Two events

$$
P(EF)=P(E)P(F)
$$

- Three events

$$
P(EFG)=P(E)P(F)P(G)
$$

$$
P(EF)=P(E)P(F)
$$

$$
P(EG)=P(E)P(G)
$$

$$
P(FG)=P(F)P(G)
$$

## Random Variables

- random variables: quantities of interest that are determined by the result of the experiment

- discrete random variables: Random variables whose set of possible values can be written either as a finite sequence *x*1, . . . , *x**n*, or as an infinite sequence *x*1, . . . 

- continuous random variables: random variables that take on a continuum of possible values

- cumulative distribution function: $F(x)=P(X\le x)$

- probability mass function: $p(a)=P(X=a)$

  for discrete random variables: $F(a)=\sum_{x\le a}p(x)$

  for continuous random variables: $F(a)=\int_{-inf}^ap(x)dx$

### Jointly Distributed Random Variables

- joint cumulative probability distribution: $F(x,y)=P(X\le x,Y\le y)$
- marginal probability distribution: $F_X(x)=F(x, inf), F_Y(y)=F(inf, y)$
- joint probability mass function: $f(x_i, y_j)=P(X=x_i, Y=y_j)$
- marginal probability mass function: $f_X(x)=\int f(x, y)dy, f_Y(y)=\int f(x, y)dx$

### Independent Random Variables

- $F(a,b)=F_X(a)F_Y(b)$
- $p(x,y)=p_X(x)p_Y(y)$

### Conditional Distribution

- $p_{X|Y}(x|y)=\frac{p(x,y)}{p_Y(y)}$

### Expectation

- Definition
  - discrete variables: $E[X]=\sum_{i=1}^n x_ip(x_i)$
  - continuous variables: $E[X]=\int xf(x)$
- Properties
  - $E[g(x)]=\sum_xg(x)p(x)$, $E[g(x)]=\int g(x)f(x)dx$
  - $E[aX+b]=aE[X]+b$
  - $E[g(X,Y)]=\sum_y \sum_xg(x,y)p(x,y)$, $E[g(X,Y)]=\int \int g(x,y)f(x, y)dxdy$
  - $E[X+Y]=E[X]+E[Y]$

### Variance

- Definition: $Var(X)=E[(X-\mu)^2]=E[X^2]-E[X]^2$
- Properties: $Var(aX+b)=a^2Var(X)$

### Covariance

- Definition: $Cov(X, Y)=E[(X-\mu_x)(Y-\mu_y)]=E[XY]-E[X]E[Y]$
- Properties:
  - $Cov(X,Y)=Cov(Y,X)$
  - $Cov(X, X)=Var(X)$
  - $Cov(aX, Y)=aCov(X,Y)$
  - $Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$
  - $Cov(\sum_i X_i, \sum_j Y_j)=\sum_i\sum_jCov(X_i, Y_j)$
  - $Var(X+Y)=Var(X)+Var(Y)+2Cov(X, Y)$
  - $Var(\sum_i X_i)=\sum_i Var(X_i)+\sum_i\sum_jCov(X_i, X_j)$
  - Independent random variables X Y => $Cov(X,Y)=0$
  - $Corr(X,Y)=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$

### Moment Generating

- Definition: $\phi(t)=E[e^{tX}]$
- Properties:
  - $\phi'(t)=E[Xe^{tX}]$
  - $\phi^n(0)=E[X^n]$
  - sum of independent random variables is just the product of the individual moment generating functions: $\phi_{X+Y}(t)=\phi_X(t)\phi_Y(t)$
  - the moment generating function uniquely determines the distribution

### Law of large numbers

- Markov's inequality:  X is a random variable that takes only nonnegative values, for any value a > 0

$$
P(X\ge a)\le \frac{E[X]}{a}
$$

- Chebyshev's inequality: X is a random variable with mean $\mu$ and variance $\sigma^2$, then for any value k > 0

$$
P\{|X-\mu|\ge k \}\le\frac{\sigma^2}{k^2}
$$

- Weak law of large numbers: X1,X2,..., be a sequence of independent and identically distributed random variables, each having mean $E[X_i] = \mu$, then for any $\epsilon$ > 0, with n -> inf

$$
P\{|\frac{X_1+...+X_n}{n}-\mu|>\epsilon\}->0
$$

## Special Random Variables

### Bernoulli

An experiment whose outcome can be classified as either a “success” or as a “failure”
$$
P(x=0)=1-p
$$

$$
P(x=1)=p
$$

$$
E[X]=p
$$

$$
Var(X)=p(1-p)
$$

### Bionomial

*n* independent trials, each of which results in a “success” with probability *p* and in a “failure” with probability 1 − *p*, If *X* represents the number of successes that occur in the *n* trials, then *X* is said to be a *binomial* random variable with parameters (*n*, *p*)
$$
P(x=i)=C_n^ip^i(1-p)^{n-i}
$$

$$
E[X]=np
$$

$$
Var(X)=np(1-p)
$$

### Possion

A random variable *X*, taking on one of the values 0, 1, 2,..., is said to be a Poisson random variable with parameter$\lambda$, $\lambda$ > 0, if its probability mass function is given by
$$
P(X=i)=e^{-\lambda}\frac{\lambda^i}{i!}
$$

$$
\phi(t)=exp(\lambda(e^t-1))
$$

$$
E[X]=\phi'(0)=\lambda
$$

$$
Var(X)=\phi''(0)-(E[X])^2=\lambda
$$

$$
\frac{P(X=i+1)}{P(X=i)}=\frac{\lambda}{i+1}
$$

The Poisson random variable may be used as an approximation for a binomial random variable with parameters (*n*, *p*) when *n* is large and *p* is small, in that case $\lambda=np$

<img src="https://i.loli.net/2021/08/04/Xy32mSYOxFnrVwz.png" alt="image-20210726235942973" style="zoom:25%;" />

### Hypergeometric

A bin contains *N* + *M* batteries, of which *N* are of acceptable quality and the other *M* are defective. A sample of size *n* is to be randomly chosen (without replacements) in the sense that the set of sampled batteries is equally likely to be any of the 􏰍*N*+*M*􏰀 subsets of *n* size *n*. If we let *X* denote the number of acceptable batteries in the sample, then
$$
P(X=i)=\frac{C_N^iC_{n-i}^M}{C_{N+M}^n}
$$

$$
E[X]=\frac{nN}{N+M}
$$

$$
Var(X)=\sum_iVar(X_i)+2\sum\sum Cov(X_i,X_j)=\frac{nNM}{(N+M)^2}(1-\frac{n-1}{N+M-1})
$$

For fixed *p*, as *N* + *M* increases to ∞, Var(*X*) converges to *np*(1 − *p*), which is the variance of a binomial random variable with parameters (*n*, *p*)

### Uniform

$$
f(x)=\frac{1}{\beta-\alpha}, if \alpha\le x \le \beta
$$

$$
E[X]=\frac{\alpha+\beta}{2}
$$

$$
Var(X)=\frac{(\beta-\alpha)^2}{12}
$$

### Normal

$$
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/2\sigma ^2}
$$

### Exponential

$$
f(x)=\lambda e^{-\lambda x}, if x\ge0
$$

$$
E[X]=1/\lambda
$$

$$
Var(X)=1/\lambda^2
$$

$$
P(X>s+t|X>t)=P(X>s)
$$

Exponentially distributed random variables are memoryless

### Chi-square

$$
X=Z_1^2+Z_2^2+...+Z_n^2
$$

$$
E[X]=n
$$

$$
Var[X]=2n
$$

### t-Distribution

$$
T_n=\frac{Z}{\sqrt{X_n^2/n}}​
$$

### F-Distribution

$$
F_{n,m}=\frac{X_n^2/n}{X_m^2/m}
$$

## Parameter Estimation

- Parametric Inference: Problems in which the form of the population distribution is specified up to a set of unknown parameters are called *parametric* inference
- Nonparametric Inference: Problems that nothing is assumed about the form of population distribution are called *nonparametric* inference problems

### Sample Statistics

If *X*1,...,*Xn* are independent random variables having a common distribution *F*, then we say that they constitute a *sample* (sometimes called a *random sample*) from the distribution *F*

- sample mean: $\bar X=\frac{X_1+...+X_n}{n}$
- expectation: $E[X]=\mu$
- variance: $Var(\bar X)=\frac{\sigma^2}{n}$

### Central Limit Theorem

$$
\frac{\bar X-\mu}{\sigma/\sqrt n}\sim N(0,1)
$$

No matter how nonnormal the underlying population distribution is, the sample mean of a sample of size at least 30 will be approximately normal