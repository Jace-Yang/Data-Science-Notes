## 聚类

### 距离度量

有序变量：欧式距离、曼哈顿距离

无序变量：VDM距离

### 原型聚类

1.k均值聚类：最小化平方误差，通过迭代的方法更新簇中心

优化目标是最小化点与类别中心点之间距离的平方和

首先选取类别个数k并初始化类别中心，计算点到类别中心的距离并将其归入距离最小的类别，重新计算类别中心并反复迭代；当某次迭代与上一次迭代后所有点的类别都没有发生改变，则聚类停止

k均值聚类最终的类别划分与初始聚类中心的选择有关，收敛到局部最小值，可以尝试选择不同初始值进行聚类

K-means与KNN的区别：K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。

2.学习向量量化：初始化一组原型向量，更新

3.高斯混合聚类：样本的生成过程由高斯混合分布给出，通过EM算法求解

高斯混合模型能提供更强的描述能力，因为聚类时数据点的从属关系不仅与近邻相关，还会依赖于类簇的形状。n维高斯分布的形状由每个类簇的协方差来决定。在协方差矩阵上添加特定的约束条件后，可能会通过GMM和k-means得到相同的结果。

### 密度聚类

DBSCAN：一簇样本点为有密度可达关系导出的最大密度相连样本集合

### 层次聚类

将数据集的每个样本堪称初始聚类簇，找出距离最近的两个聚类簇进行合并，重复该过程直到达到预设的聚类簇个数

## EM算法

对于含有隐变量的模型，极大似然法不方便求解，通过Jensen不等式进行放缩，极大化似然函数的下界，E步求期望，M步求期望的极大值来得到新的参数，这样反复迭代后，求出最优解



### 