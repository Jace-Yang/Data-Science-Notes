## Logistic回归

Logistic Regression的loss是什么

logistic regression的损失函数为交叉熵损失函数
$$
J(\theta)=-[ylog(h(x))+(1-y)log(1-h(x))]
$$

## KNN

1.KNN算法

输入训练数据集，输出实例x所属的类y

(1)根据给定的距离算法，在训练集找出与x最邻近的k个点

(2)根据这些点多数点所属类别确定这个点的类别

KNN没有显式的学习过程，没有模型参数

2.KNN距离
$$
L_p(x_i,x_j)=(\sum|x_i-x_j|^p)^{1/p}
$$
P=1，称为曼哈顿距离

P=2，称为欧式距离

P=无穷时，是每个坐标距离的最大值

不同距离度量确定的最近邻点是不同的

 3.k值选择

k值较小，训练误差小，易发生过拟合。K=1时训练误差为零

k值较大，训练误差大，模型变简单，易发生欠拟合。K=N时类似于少数服从多数

在应用过程中，通过交叉验证(cross-validation)取验证误差最小时对应的k

4.KNN的实现：kd树

kd树是一种二叉树，表示对k维空间的一个划分

kd树的构造：

(1)构造根节点，使其对应于包含所有数据的k维空间超矩形区域，选择x1为坐标轴，以所有数据点x1值的中位数作为切分点，将超矩形区域分为两个子区域并生成根节点的左右子节点，分别对应小于和大于x1坐标的子区域，落在切分平面上的点保留在根节点

(2)对深度为j的节点，重复上述过程并生成j+1层的子节点直到两个子区域没有实例存在时停止

kd树的搜索：

(1)找到包含目标点x的叶节点（从根节点向下遍历）

(2)以此叶节点为当前最近点

(3)递归向上回退，先检查父节点是否更近，若是则更新最近点，若不是则检查另一子节点对应区域是否相交，若相交则在另一子节点对应区域查找，若不相交则回退至父节点

(4)按步骤3递归进行回退直到根节点

kd树搜索适用于训练实例树大于空间维度时的搜索，平均计算复杂度时O(logN)

## 决策树

1.概念

决策树模型学习步骤：特征选择、决策树生成、决策树修剪

性质：决策树的路径互斥并且完备，每一个实例都被一条路径或者一条规则所覆盖，且仅被一条路径或规则所覆盖

学习目标：根据给定的训练数据构建模型，使其能够对实例进行正确分类，得到一个与训练数据矛盾较小，且具有较强泛化能力的模型 

2.特征选择：

选择准则：信息增益或信息增益比

随机变量X的熵：$H(X)=-\sum p_i \log p_i$

条件熵：$H(Y|X)=\sum p_iH(Y|X=x_i)$

信息增益：$g(D,A)=H(D)-H(D|A)$ 信息增益表示由于特征A而使得对数据集D的分类的不确定性减少程度

信息增益比：$g_R(D,A)=g(D,A)/H(D)$

3.决策树的生成

ID3算法：从根节点开始，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，由该特征的不同取值建立子节点；再对子节点递归调用上述方法，构建决策树，直到所有节点信息增益小于某个阈值为止

C4.5算法：与ID3算法类似，选择信息增益比代替信息增益作为选择标准 

4.决策树的剪枝

在决策树的学习中将已生成的树进行简化的过程称为剪枝

决策树的损失函数如下，T为叶节点个数，N为第t个叶节点样本数，加L1正则化项
$$
C(T)=\sum N_tH_t(T)+\alpha |T|
$$
在具体操作过程中，考虑剪枝前后树的熵，若剪枝后熵小于剪枝前熵，则进行剪枝，这种算法可以用动态规划的方法实现

5.CART算法

分类树与回归树算法，CART假设决策树是二叉树，内部节点特征的取值为是和否，左分支为取值为是的分支，右分支为取值为否的分支，使用基尼系数来选择最优变量的最优拆分点

回归树：回归树的训练过程采用启发式的方法选择第i个变量和他的切分点s，通过最小化误差函数的方法选择最优切分变量和切分点，每个节点样本的均值作为测试样本的回归预测值

(1)选择最优切分变量与切分点
$$
min_{j,s}[min \sum_{R_1(j,s)}(y_i-c_1)^2+min\sum_{R_2(j,s)}(y_i-c_2)^2]
$$
遍历j，对固定的切分变量j，扫描切分点，使得上式达到最小，得到(j,s)

(2)用选定的(j,s)划分区域并决定输出值

(3)继续对子区域调用步骤1、2

(4)输入空间划分为M个区域，生成决策树

分类树：计算现有特征对数据集的基尼指数，在所有特征和切分点中选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，每个节点样本的类别情况投票决定测试样本的类别

基尼指数：$Gini(p)=\sum p_k(1-p_k)$

基尼指数值越大，样本集合不确定性越大，与熵类似

(1)训练数据集为D，计算现有特征所有可能取值对数据集的基尼指数

(2)在所有可能特征和它们的切分点a中选择基尼指数最小的特征及其切分点作为最优特征与最优切分点，并生成两个子节点

(3)对子节点递归调用1、2

(4)生成决策树

CART剪枝：从生成算法产生的决策树底端开始不断剪枝，直到根节点，形成子树序列，然后通过交叉验证集对子树序列进行测试，选择最优子树

6.决策树的剪枝

可以通过预剪枝或者后剪枝的方法预防过拟合

预剪枝策略：

定义一个高度，当决策树达到该高度时就可以停止决策树的生长

达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长

定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长

定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长

后剪枝策略：删除一些子树，然后用其叶子节点代替

REP方法是一种比较简单的后剪枝的方法，在该方法中，可用的数据被分成两个样例集合：一个训练集用来形成学习到的决策树，一个分离的验证集用来评估这个决策树在后续数据上的精度，确切地说是用来评估修剪这个决策树的影响

PEP,悲观错误剪枝,悲观错误剪枝法是根据剪枝前后的错误率来判定子树的修剪

7.决策树的正则化

通过设置最大深度，节点分裂阈值等方法来完成正则化

## 朴素贝叶斯

1.贝叶斯公式
$$
P(A|B)=\frac{P(A,B)}{P(B)}=\frac{P(B|A)P(A)}{P(B)}
$$
学习目标：联合概率分布$P(A,B)$

加入先验知识$P(A)$

最大似然估计与最大后验估计

MLE: 最大化似然函数

MAP: 最大化后验概率，估计得到参数的后验分布

伯努利分布：MAP先验分布选择beta分布，后验与先验分布形式一致

多项式分布：MAP先验分布选择dirichlet分布 

3.朴素贝叶斯分类器
$$
P(Y|X_1,X_2,...,X_N)=\frac{P(X_1,X_2,...,X_N|Y)P(Y)}{P(X_1,X_2,...,X_N)}
$$
左侧需要参数过多，为$2^{n+1}-1$个

朴素贝叶斯：假设feature是独立的

目标函数：$arg max P(Y=y_k)\pi P(X_i|Y=y_k)$

4.朴素贝叶斯与逻辑回归

朴素贝叶斯相比于逻辑回归收敛更快，需要样本更少

数据量足够大时，逻辑回归效果更好，考虑了不同变量之间的相关关系

## 支持向量机SVM

1.线性可分支持向量机

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。

分离超平面：$wx+b=0$

分类决策函数：$f(x)=sign(wx+b)$

函数间隔：$y_i(wx_i+b)$

几何间隔：$\frac{y_i}{||w||}(wx_i+b)$

将间隔最大化问题转化为最优化问题：
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 0, i=1,2,...,N
$$

间隔最大化算法求出的分离超平面解是唯一的

支持向量：训练集中离分离超平面距离最近的样本点

间隔：正负支持向量与分离超平面之间距离之和，为$2/||w||$

对偶问题：构造拉格朗日函数求解
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N \alpha_iy_i(wx_i+b)+\sum_{i=1}^N \alpha_i
$$
2.线性不可分支持向量机

相比于线性可分支持向量机，引入松弛变量$\epsilon_i$，允许存在错误分类的点。通过构造拉格朗日函数求解约束优化问题
$$
min \frac{1}{2}||w||^2
$$

$$
s.t. y_i(wx_i+b)-1\ge 1-\epsilon_i, i=1,2,...,N
$$

3.非线性支持向量机

对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机

在非线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换**，**而是用核函数替换当中的内积。

$$
K(x,z)=\phi (x)\sdot\phi(z)
$$
![img](https://pic1.zhimg.com/80/v2-31c10f29156c5acedafda70524d8eb00_1440w.jpg)

4.怎么把SVM的output按照概率输出

SVM分类器能输出（测试）样本和决策边界的距离，可以把这个距离当做一个置信度数值。可以用Logistic Regression把SVM输出的置信度数值校准为概率值