## 降维

1.优势

高维数据可视化

节省时间、内存

更好的泛化能力

去除噪音

2.矩阵分解

B为新空间的基，C为A在新空间的坐标

3.主成分分析

N个样本，P个维度，组成数据矩阵

每个维度进行去中心化

4.LDA

5.NMF

1.Explain PCA

主成分分析是一种统计分析、简化数据集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分

2.LDA是什么，假设是什么

LDA算法的思想是将数据投影到低维空间之后，使得同一类数据尽可能的紧凑，不同类的数据尽可能分散。与PCA不同的是，LDA是一种监督学习方法

## 聚类

### 距离度量

有序变量：欧式距离、曼哈顿距离

无序变量：VDM距离

### 原型聚类

1.k均值聚类：最小化平方误差，通过迭代的方法更新簇中心

优化目标是最小化点与类别中心点之间距离的平方和

首先选取类别个数k并初始化类别中心，计算点到类别中心的距离并将其归入距离最小的类别，重新计算类别中心并反复迭代；当某次迭代与上一次迭代后所有点的类别都没有发生改变，则聚类停止

k均值聚类最终的类别划分与初始聚类中心的选择有关，收敛到局部最小值，可以尝试选择不同初始值进行聚类

K-means与KNN的区别：K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。

2.学习向量量化：初始化一组原型向量，更新

3.高斯混合聚类：样本的生成过程由高斯混合分布给出，通过EM算法求解

高斯混合模型能提供更强的描述能力，因为聚类时数据点的从属关系不仅与近邻相关，还会依赖于类簇的形状。n维高斯分布的形状由每个类簇的协方差来决定。在协方差矩阵上添加特定的约束条件后，可能会通过GMM和k-means得到相同的结果。

### 密度聚类

DBSCAN：一簇样本点为有密度可达关系导出的最大密度相连样本集合

$\epsilon$邻域：对于每个样本，它周围距离 $\epsilon$以内的样本的集合

核心对象：$\epsilon$邻域中样本数目大于MinPts的样本

密度直达：核心对象$\epsilon$邻域内的所有样本由这个核心对象密度直达

密度可达：核心对象密度直达具有传递性，由这种传递性连接的两个核心对象称为密度可达

密度相连：核心对象$\epsilon$邻域里的两个点密度相连

聚类过程：任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止

### 层次聚类

将数据集的每个样本堪称初始聚类簇，找出距离最近的两个聚类簇进行合并，重复该过程直到达到预设的聚类簇个数

## EM算法

对于含有隐变量的模型，极大似然法不方便求解，通过Jensen不等式进行放缩，极大化似然函数的下界，E步求期望，M步求期望的极大值来得到新的参数，这样反复迭代后，求出最优解