## Regression回归

1.Linear Regression的基础假设是什么

线性性：因变量和自变量满足线性关系

独立性（误差项）：误差项之间相互独立

独立性（自变量）：各个自变量之间相互独立

正态性：误差项应呈正态分布

同方差性：误差项的方差为常数

2.what will happen when we have correlated variables, how to solve

删除变量：这个方法一般不推荐使用，因为删除变量会导致异方差增大

增加样本容量：增大样本量，减小多重共线性

变换模型：对数据求差分；计算相对指标；相关变量做线性组合，例如使用主成分分析等方法降维

逐步回归：常用方法，添加删除变量之后做可决系数、F检验和T检验来确定是否增加或者剔除变量，若果增加变量对这些指标的影响较小，也认为指标为多余的，如果增加指标引起R和F的变动且通不过T检验，说明存在共线性

岭回归：Ridge regression加入正则化项，减小自变量系数，减弱多重共线性

3.explain regression coefficient

回归分析只能得出相关关系，不能判断因果关系

当其他变量保持不变或控制其他变量不变时$X$每改变一个单位时因变量$Y$的平均变化量

4.what is the relationship between minimizing squared error and maximizing the likelihood

两种不同的求最优解的方法，最终计算出的结果是一致的

最小二乘法(OLS)基于频率派概率思想，求真值与估计值之间差值最小对应的参数

最大似然(MLE)基于贝叶斯思想，将参数看作概率分布，通过最大化似然函数来求参数

5.if the relationship between y and x is no linear, can linear regression solve that

可以添加高次变量，以及不同变量之间的组合，例如$x_1^2$ $x_1x_2$等交互变量作为自变量
