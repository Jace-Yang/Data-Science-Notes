## 线性模型

### 一元线性回归

**最小二乘法：**最小化均方误差
$$
J=\frac{1}{m}\sum_{i=1}^{m}(y_i-f(x_i))^2=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

**极大似然估计：**最大化极大似然函数等价于最小二乘
$$
L(\theta)=\prod_{i=1}^{n} P(x_i;\theta)
$$

$$
y=wx+b+\epsilon
$$

$$
p(\epsilon)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{\epsilon^2}{2\sigma^2})
$$

$$
p(y)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{(y-(wx+b))^2}{2\sigma^2})
$$

$$
\ln L(w,b)=\sum_{i=1}^m\ln p(y_i)=m\ln \frac{1}{\sqrt {2\pi}\sigma} - \frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-wx_i-b)^2
$$

$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

**凸函数证明：**

凸集：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$\alpha x+(1-\alpha)y \in D$则称D为凸集

凸函数：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$f(\alpha x_1+(1-\alpha)x_2)\le \alpha f(x_1)+(1-\alpha)f(x_2)$称$f$为凸函数

多元函数的一阶导数：对每个分量求偏导数，构成梯度向量

多元函数的二阶导数：Hessain矩阵，一阶导数组合，形成二阶导数矩阵

凸函数定理：若$f(x)$的Hessian矩阵是半正定的，则$f(x)$为凸函数

半正定矩阵的判定定理：实对称矩阵的所有顺序主子式均非负，则矩阵为半正定矩阵

通过上述定理可证明一元线性回归的目标函数是凸函数

**求解最优化问题：**

凸充分性定理：对于凸函数来说，全局最小值的充要条件是一阶导等于零
$$
\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-wx_i)+2b)=0
$$

$$
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)=\bar y-w\bar x
$$

$$
\frac{\partial J}{\partial w}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-b)x_i+2x_i^2w)=0
$$

$$
w\sum_{i=1}^mx_i^2=\sum_{i=1}^{m}(y_i-b)x_i=\sum_{i=1}^{m}(y_i-(\bar y-w\bar x))x_i
$$

$$
w=\frac{\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx_i^2-\bar x\sum_{i=1}^mx_i}
$$

### 多元线性回归

矩阵微分公式：https://en.wikipedia.org/wiki/Matrix_calculus

$\bold X$是$M * N$维的矩阵，$M$是样本数量，$N$是自变量维度
$$
f(\bold{x_i})=\bold w^T \bold{x_i} + b=\hat{\bold w}^T\hat{\bold{x_i}}
$$

$$
J(\bold w)=(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\bold {\hat w} = argmin(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=\frac{\partial}{\partial \hat{\bold w}}[-\bold{yX\hat w}-\bold{\hat w^T X^Ty}+\bold{\hat w^TX^TX\hat w}]
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=2\bold X^T(\bold X\bold{\hat w}-\bold y)
$$

$$
\frac{\partial^2 J}{\partial \hat{\bold w}^2}=2\bold X^T\bold X
$$

$$
\bold{\hat w}=(\bold{X^TX})^{-1}\bold{X^T}\bold y
$$

### Logistic Regression

在线性模型的基础上加一个映射函数来实现分类功能（从实数域R映射到[0,1]）

**极大似然估计：**
$$
p_1=\frac{1}{1+e^{-(\bold w^T \bold x+b)}}=\frac{e^{\bold w^T \bold x+b}}{1+e^{\bold w^T \bold x+b}}
$$

$$
p_0=1-p_1=\frac{1}{1+e^{\bold w^T \bold x+b}}
$$

$$
p(y)=y\cdot p_1+(1-y)\cdot p_0
$$

$$
Loss = -lnL=\sum_{i=1}^mln(y_i\cdot p_1+(1-y_i)\cdot p_0)=\sum_{i=1}^m(y_i\bold \beta^T\bold {\hat x_i}-ln(1+e^{\beta^T\bold{\hat x_i}}))
$$

**信息论：**

信息熵：度量随机变量的不确定性，信息熵越大越不确定，公式为$H(X)=-\sum_x p(x)\log p(x)$

KL散度：度量两个分布的差异，也叫做相对熵，用来度量提议分布$q(x)$与真实分布$p(x)$的差距
$$
D_{KL}(p||q)=\sum_xp(x)log(\frac{p(x)}{q(x)})=\sum_xp(x)log(p(x))-\sum_xp(x)log(q(x))
$$
交叉熵：真实分布$p(x)$为常数，KL散度的后半部分称作交叉熵，公式是$-\sum_xp(x)\log q(x)$
$$
CrossEntropy=-y_i\log p_1-(1-y_i)\log p_0
$$

$$
Loss = \sum_{i=1}^m(y_i\bold \beta^T\bold {\hat x_i}-ln(1+e^{\beta^T\bold{\hat x_i}}))
$$

### LDA线性判别分析

目标：异类样本中心尽可能远，同类样本方差尽可能小
$$
max||\bold w^T\mu_0-\bold w^T\mu_1||_2^2
$$

$$
min\bold w^T\bold \sum_0\bold w
$$

$$
min\bold w^T\bold \sum_1\bold w
$$

$$
J = \frac{||\bold w^T\mu_0-\bold w^T\mu_1||_2^2}{\bold w^T\bold \sum_0\bold w+\bold w^T\bold \sum_1\bold w}=\frac{\bold w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\bold w}{\bold w^T(\sum_0+\sum_1)\bold w}=\frac{\bold w^T\bold S_b \bold w}{\bold w^T\bold S_w \bold w}
$$

固定w的模长，求最优化问题来求解w的方向；通常固定分母大小，求负分子的最小化问题

带约束的优化问题可以通过拉格朗日乘子法求解
$$
L(\bold w, \lambda)=-\bold w^T\bold S_b\bold w+\lambda(\bold w^T\bold S_w\bold w-1)
$$

$$
\frac{\partial L}{\partial \bold w}=-2\bold S_b\bold w+2\lambda \bold S_w \bold w=0
$$

$$
\bold w = \frac{\gamma}{\lambda}\bold S_w^{-1}(\mu_0-\mu_1)=\bold S_w^{-1}(\mu_0-\mu_1)
$$

不关心$\bold w$的大小，只关心方向，可以令$\gamma = \lambda$，求解完毕

广义特征值：$\bold A\bold x=\lambda \bold B\bold x$，$\lambda$称作$\bold A$相对于$\bold B$的广义特征值

广义瑞利商：$\frac{\bold x^T \bold A \bold x}{\bold x^T\bold B \bold x}$，称作$\bold A$相对于$\bold B$的广义瑞利商

广义瑞利商的最小值等于最小广义特征值，最大值等于最大广义特征值

## 树模型

### ID3决策树

随机变量的信息熵：$H(x)=-\sum_x p(x)\log p(x)$

信息熵最大时，不确定性最高；信息熵等于零时，不确定性最低

对于随机变量$X$来说，当$X$的某个取值概率为1时信息熵最小；当$X$的每个取值概率均等时信息熵最大

条件熵：$H(Y|X)=\sum_x p(x)H(Y|X=x)$

信息增益：在已知属性a的取值后，y的不确定性减少的量
$$
Gain(D,a)=H(D)-\sum_v\frac{|D^v|}{D}H(D^v)
$$
ID3决策树以信息增益为准则来选择划分属性的决策树

### C4.5决策树

信息增益准则对可能取值数目较多的属性有所偏好，选择信息增益率代替信息增益
$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\
$$

$$
IV(a)=-\sum_v \frac{|D^v|}{|D|}\log \frac{|D^v|}{|D|}
$$

a的可能取值个数越大，固有值$IV(a)$越大，但这种方法对于取值树木少的属性有所偏好

通常情况，C4.5算法先使用信息增益选出所有高于平均信息增益的特征，再从中选择信息增益率最大的

### CART决策树

基尼值：从样本集合中随机抽取两个样本，类别标记不一致的概率
$$
Gini(D)=\sum_kp_k(1-p_k)=1-\sum_kp_k^2
$$
基尼指数：类似于基尼值的条件熵
$$
Gini\_index(D,a)=\sum_v\frac{|D^v|}{|D|}Gini(D^v)
$$
与ID3决策树与C4.5决策树不同的是，CART决策树是一个二叉树，在用属性划分数据集时，将数据集分为等于与不等于两部分，之后选择基尼指数最小的属性以及对应的取值作为最优划分属性和划分点

