## 线性模型

### 一元线性回归

**最小二乘法：**最小化均方误差
$$
J=\frac{1}{m}\sum_{i=1}^{m}(y_i-f(x_i))^2=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$


$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

**极大似然估计：**最大化极大似然函数等价于最小二乘
$$
L(\theta)=\prod_{i=1}^{n} P(x_i;\theta)
$$


$$
y=wx+b+\epsilon
$$


$$
p(\epsilon)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{\epsilon^2}{2\sigma^2})
$$


$$
p(y)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{(y-(wx+b))^2}{2\sigma^2})
$$


$$
\ln L(w,b)=\sum_{i=1}^m\ln p(y_i)=m\ln \frac{1}{\sqrt {2\pi}\sigma} - \frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-wx_i-b)^2
$$

$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

**凸函数证明：**

凸集：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$\alpha x+(1-\alpha)y \in D$则称D为凸集

凸函数：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$f(\alpha x_1+(1-\alpha)x_2)\le \alpha f(x_1)+(1-\alpha)f(x_2)$称$f$为凸函数

多元函数的一阶导数：对每个分量求偏导数，构成梯度向量

多元函数的二阶导数：Hessain矩阵，一阶导数组合，形成二阶导数矩阵

凸函数定理：若$f(x)$的Hessian矩阵是半正定的，则$f(x)$为凸函数

半正定矩阵的判定定理：实对称矩阵的所有顺序主子式均非负，则矩阵为半正定矩阵

通过上述定理可证明一元线性回归的目标函数是凸函数

**求解最优化问题：**

凸充分性定理：对于凸函数来说，全局最小值的充要条件是一阶导等于零
$$
\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-wx_i)+2b)=0
$$

$$
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)=\bar y-w\bar x
$$

$$
\frac{\partial J}{\partial w}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-b)x_i+2x_i^2w)=0
$$

$$
w\sum_{i=1}^mx_i^2=\sum_{i=1}^{m}(y_i-b)x_i=\sum_{i=1}^{m}(y_i-(\bar y-w\bar x))x_i
$$

$$
w=\frac{\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx_i^2-\bar x\sum_{i=1}^mx_i}
$$

### 多元线性回归

矩阵微分公式：https://en.wikipedia.org/wiki/Matrix_calculus

$\bold X$是$M * N$维的矩阵，$M$是样本数量，$N$是自变量维度
$$
f(\bold{x_i})=\bold w^T \bold{x_i} + b=\hat{\bold w}^T\hat{\bold{x_i}}
$$

$$
J(\bold w)=(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\bold {\hat w} = argmin(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=\frac{\partial}{\partial \hat{\bold w}}[-\bold{yX\hat w}-\bold{\hat w^T X^Ty}+\bold{\hat w^TX^TX\hat w}]
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=2\bold X^T(\bold X\bold{\hat w}-\bold y)
$$

$$
\frac{\partial^2 J}{\partial \hat{\bold w}^2}=2\bold X^T\bold X
$$

$$
\bold{\hat w}=(\bold{X^TX})^{-1}\bold{X^T}\bold y
$$

### Logistic Regression

在线性模型的基础上加一个映射函数来实现分类功能（从实数域R映射到[0,1]）

**极大似然估计：**
$$
p_1=\frac{1}{1+e^{-(\bold w^T \bold x+b)}}=\frac{e^{\bold w^T \bold x+b}}{1+e^{\bold w^T \bold x+b}}
$$

$$
p_0=1-p_1=\frac{1}{1+e^{\bold w^T \bold x+b}}
$$

$$
p(y)=y\cdot p_1+(1-y)\cdot p_0
$$

$$
Loss = -lnL=\sum_{i=1}^mln(y_i\cdot p_1+(1-y_i)\cdot p_0)=\sum_{i=1}^m(y_i\bold \beta^T\bold {\hat x_i}-ln(1+e^{\beta^T\bold{\hat x_i}}))
$$

**信息论：**

信息熵：度量随机变量的不确定性，信息熵越大越不确定，公式为$H(X)=-\sum_x p(x)\log p(x)$

KL散度：度量两个分布的差异，也叫做相对熵，用来度量提议分布$q(x)$与真实分布$p(x)$的差距
$$
D_{KL}(p||q)=\sum_xp(x)log(\frac{p(x)}{q(x)})=\sum_xp(x)log(p(x))-\sum_xp(x)log(q(x))
$$
交叉熵：真实分布$p(x)$为常数，KL散度的后半部分称作交叉熵，公式是$-\sum_xp(x)\log q(x)$
$$
CrossEntropy=-y_i\log p_1-(1-y_i)\log p_0
$$

$$
Loss = \sum_{i=1}^m(y_i\bold \beta^T\bold {\hat x_i}-ln(1+e^{\beta^T\bold{\hat x_i}}))
$$

### LDA线性判别分析

目标：异类样本中心尽可能远，同类样本方差尽可能小
$$
max||\bold w^T\mu_0-\bold w^T\mu_1||_2^2
$$

$$
min\bold w^T\bold \sum_0\bold w
$$

$$
min\bold w^T\bold \sum_1\bold w
$$

$$
J = \frac{||\bold w^T\mu_0-\bold w^T\mu_1||_2^2}{\bold w^T\bold \sum_0\bold w+\bold w^T\bold \sum_1\bold w}=\frac{\bold w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\bold w}{\bold w^T(\sum_0+\sum_1)\bold w}=\frac{\bold w^T\bold S_b \bold w}{\bold w^T\bold S_w \bold w}
$$

固定w的模长，求最优化问题来求解w的方向；通常固定分母大小，求负分子的最小化问题

带约束的优化问题可以通过拉格朗日乘子法求解
$$
L(\bold w, \lambda)=-\bold w^T\bold S_b\bold w+\lambda(\bold w^T\bold S_w\bold w-1)
$$

$$
\frac{\partial L}{\partial \bold w}=-2\bold S_b\bold w+2\lambda \bold S_w \bold w=0
$$

$$
\bold w = \frac{\gamma}{\lambda}\bold S_w^{-1}(\mu_0-\mu_1)=\bold S_w^{-1}(\mu_0-\mu_1)
$$

不关心$\bold w$的大小，只关心方向，可以令$\gamma = \lambda$，求解完毕

广义特征值：$\bold A\bold x=\lambda \bold B\bold x$，$\lambda$称作$\bold A$相对于$\bold B$的广义特征值

广义瑞利商：$\frac{\bold x^T \bold A \bold x}{\bold x^T\bold B \bold x}$，称作$\bold A$相对于$\bold B$的广义瑞利商

广义瑞利商的最小值等于最小广义特征值，最大值等于最大广义特征值

