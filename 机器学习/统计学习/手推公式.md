## 线性模型

### 一元线性回归

最小二乘法：最小化均方误差
$$
J=\frac{1}{m}\sum_{i=1}^{m}(y_i-f(x_i))^2=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

极大似然估计：最大化极大似然函数等价于最小二乘
$$
L(\theta)=\prod_{i=1}^{n} P(x_i;\theta)
$$

$$
y=wx+b+\epsilon
$$

$$
p(\epsilon)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{\epsilon^2}{2\sigma^2})
$$

$$
p(y)=\frac{1}{\sqrt {2\pi}\sigma}exp(-\frac{(y-(wx+b))^2}{2\sigma^2})
$$

$$
\ln L(w,b)=\sum_{i=1}^m\ln p(y_i)=m\ln \frac{1}{\sqrt {2\pi}\sigma} - \frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-wx_i-b)^2
$$

$$
w,b=argmin_{(w,b)}\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i-b)^2
$$

凸函数证明：

凸集：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$\alpha x+(1-\alpha)y \in D$则称D为凸集

凸函数：对任意的$x,y \in D$与任意$\alpha \in [0,1]$都有$f(\alpha x_1+(1-\alpha)x_2)\le \alpha f(x_1)+(1-\alpha)f(x_2)$称$f$为凸函数

多元函数的一阶导数：对每个分量求偏导数，构成梯度向量

多元函数的二阶导数：Hessain矩阵，一阶导数组合，形成二阶导数矩阵

凸函数定理：若$f(x)$的Hessian矩阵是半正定的，则$f(x)$为凸函数

半正定矩阵的判定定理：实对称矩阵的所有顺序主子式均非负，则矩阵为半正定矩阵

通过上述定理可证明一元线性回归的目标函数是凸函数

求解最优化问题：

凸充分性定理：对于凸函数来说，全局最小值的充要条件是一阶导等于零
$$
\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-wx_i)+2b)=0
$$

$$
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i)=\bar y-w\bar x
$$

$$
\frac{\partial J}{\partial w}=\frac{1}{m}\sum_{i=1}^m(-2(y_i-b)x_i+2x_i^2w)=0
$$

$$
w\sum_{i=1}^mx_i^2=\sum_{i=1}^{m}(y_i-b)x_i=\sum_{i=1}^{m}(y_i-(\bar y-w\bar x))x_i
$$

$$
w=\frac{\sum_{i=1}^my_ix_i-\bar y\sum_{i=1}^mx_i}{\sum_{i=1}^mx_i^2-\bar x\sum_{i=1}^mx_i}
$$

### 多元线性回归

矩阵微分公式：https://en.wikipedia.org/wiki/Matrix_calculus

$\bold X$是$M * N$维的矩阵，$M$是样本数量，$N$是自变量维度
$$
f(\bold{x_i})=\bold w^T \bold{x_i} + b=\hat{\bold w}^T\hat{\bold{x_i}}
$$

$$
J(\bold w)=(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\bold {\hat w} = argmin(\bold y - \bold X \bold{\hat w})^T(\bold y - \bold X \bold{\hat w})
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=\frac{\partial}{\partial \hat{\bold w}}[-\bold{yX\hat w}-\bold{\hat w^T X^Ty}+\bold{\hat w^TX^TX\hat w}]
$$

$$
\frac{\partial J}{\partial \hat{\bold w}}=2\bold X^T(\bold X\bold{\hat w}-\bold y)
$$

$$
\frac{\partial^2 J}{\partial \hat{\bold w}^2}=2\bold X^T\bold X
$$

$$
\bold{\hat w}=(\bold{X^TX})^{-1}\bold{X^T}\bold y
$$

