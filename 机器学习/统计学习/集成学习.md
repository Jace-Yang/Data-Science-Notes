## Ensemble Learning集成学习

1.difference between bagging and boosting

样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整

样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

预测函数：

Bagging：所有预测函数的权重相等

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

2.gbdt和random forest 区别，pros and cons

随机森林采用的bagging思想，而GBDT采用的boosting思想

组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成

组成随机森林的树可以并行生成；而GBDT只能是串行生成

对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来

随机森林对异常值不敏感；GBDT对异常值非常敏感

随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成

随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能

3.will random forest help reduce bias or variance/why random forest can help reduce variance

随机森林随机选择数据集和特征，特征数量小于总特征，多个分类器的平均结果就是最终预测的结果，而多个分类器的方差相似，去平均后方差为原来的1/n，所以随机森林可以减小方差，但是均值与单个模型的均值相同，所以不能减小偏差

通常来说boosting是在优化loss function，在降低loss，那么很显然，这在很大程度上是减少bias。
而bagging，之所以进行bagging，是希望模型能够具有更好的鲁棒性，也就是稳定性，希望避免过拟合，显然这就是在减少variance