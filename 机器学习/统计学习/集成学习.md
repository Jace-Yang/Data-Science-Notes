## Ensemble Learning集成学习

1.集成学习分类

一般来说集成学习可以分为三大类：

- 用于减少方差的bagging
- 用于减少偏差的boosting
- 用于提升预测结果的stacking

集成学习方法也可以归为如下两大类：

- 串行集成方法，这种方法串行地生成基础模型（如AdaBoost）。串行集成的基本动机是利用基础模型之间的依赖。通过给错分样本一个较大的权重来提升性能。
- 并行集成方法，这种方法并行地生成基础模型（如Random Forest）。并行集成的基本动机是利用基础模型的独立性，因为通过平均能够较大地降低误差。

2.Bagging

Bagging使用装袋采样来获取数据子集训练基础学习器。通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。Bagging中基学习器的数据是从所有数据中随机选择的，用于学习的特征也是随机选择一部分

在随机森林中，每个树模型都是装袋采样训练的。另外，特征也是随机选择的，最后对于训练好的树也是随机选择的，这种处理的结果是随机森林的偏差增加的很少，而由于弱相关树模型的平均，方差也得以降低，最终得到一个方差小，偏差也小的模型。

3.Boosting

Boosting指的是通过算法集合将弱学习器转换为强学习器。boosting的主要原则是训练一系列的弱学习器，所谓弱学习器是指仅比随机猜测好一点点的模型，例如较小的决策树，训练的方式是**利用加权的数据**。在训练的早期对于错分数据给予较大的权重。 

AdaBoost第一个分类器y1(x)是用相等的权重系数进行训练的。在随后的boosting中，错分的数据权重系数将会增加，正确分类的数据权重系数将会减小。

梯度树提升（Gradient Tree Boosting）是一个boosting算法在损失函数上的泛化。能够用于分类和回归问题。Gradient Boosting采用串行方式构建模型。

4.Stacking

Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。

基础模型通常包含不同的学习算法，因此stacking通常是异质集成。

4.difference between bagging and boosting

样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整

样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大

预测函数：

Bagging：所有预测函数的权重相等

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重

并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果

## 树模型Boosting

### GBDT

GBDT是通过Boosting的思路来对决策树模型进行提升。GBDT采用加法模型，每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差，因此每棵分类回归树的深度较浅，最终的总分类器在每轮训练的弱分类器加权求和得到。

GBDT使用的弱分类器是CART回归树，但是可以用于分类问题也可以用于回归问题，分类与回归所对应的损失函数不同

• 分类问题：$Loss = \sum_i -y_i\log(p_i)-(1-y_i)\log(1-p_i)$

• 回归问题：$Loss = \sum_i (y_i - \hat y_i)^2$

### XGBoost

- 首先，对所有特征都按照特征的数值进行预排序。
- 其次，在遍历分割点的时候用O(data)的代价找到一个特征上的最好分割点。
- 最后，找到一个特征的分割点后，将数据分裂成左右子节点。
- 优点：

算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而**XGBoost损失函数对误差部分做二阶泰勒展开**，更加准确。算法本身的优化是我们后面讨论的重点。

算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。

算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。

- 缺点：

首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。

其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。

最后，对 cache 优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的 cache miss。  

### LightGBM

**直方图算法**

基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

优点：

内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的1/8

计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)

**带深度限制的 Leaf-wise 的叶子生长策略**

Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。  

**直方图加速**

一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍

**LightGBM并行优化**

LightGBM 还具有支持高效并行的优点。LightGBM 原生支持并行学习，目前支持**特征并行**和**数据并行**的两种。

- 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。
- 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。

在特征并行算法中，通过在本地保存全部数据避免对数据切分结果的通信；

在数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。基于投票的数据并行则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。

## 树模型Bagging

### Random Forest

随机森林是采用Bagging的思想，构建多棵决策树，再通过多棵决策树模型平均的方法构建学习器。对于不同树的样本，通过bootstrap采样的方法得到，不同树采用的特征，也通过bootstrap采样得到

随机森林随机选择数据集和特征，特征数量小于总特征，多个分类器的平均结果就是最终预测的结果，而多个分类器的方差相似，去平均后方差为原来的1/n，所以随机森林可以减小方差，但是均值与单个模型的均值相同，所以不能减小偏差

GBDT和Random Forest区别：

• 随机森林采用的bagging思想，而GBDT采用的boosting思想

• 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成

• 组成随机森林的树可以并行生成；而GBDT只能是串行生成

• 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来

• 随机森林对异常值不敏感；GBDT对异常值非常敏感

• 随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成

• 随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能
